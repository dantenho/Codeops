"""
Token usage metrics models for tracking and optimizing LLM token consumption.

This module provides comprehensive token tracking across training sessions,
enabling cost monitoring, efficiency analysis, and optimization opportunities.
"""

from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional
from pydantic import BaseModel, Field, computed_field


class TokenSource(str, Enum):
    """Source of token consumption."""

    PROMPT = "prompt"  # Base system/instruction prompt
    CONTEXT = "context"  # Retrieved context from RAG
    USER_INPUT = "user_input"  # User's query/intent
    COMPLETION = "completion"  # Model's generated output
    CACHED = "cached"  # Reused from cache


class TokenMetrics(BaseModel):
    """Comprehensive token usage metrics for a single operation."""

    # Identifiers
    session_id: str = Field(description="Training session ID")
    agent_id: str = Field(description="Agent performing the operation")
    operation_id: str = Field(description="Unique operation identifier")
    timestamp: datetime = Field(default_factory=datetime.utcnow)

    # Token Counts by Source
    prompt_tokens: int = Field(default=0, ge=0, description="Base prompt tokens")
    context_tokens: int = Field(default=0, ge=0, description="RAG-retrieved context tokens")
    user_input_tokens: int = Field(default=0, ge=0, description="User query tokens")
    completion_tokens: int = Field(default=0, ge=0, description="Generated output tokens")
    cached_tokens: int = Field(default=0, ge=0, description="Reused cached tokens")

    # Quality Metrics
    output_quality_score: float = Field(
        default=0.0,
        ge=0.0,
        le=100.0,
        description="Quality score of generated output (0-100)"
    )
    context_utilization_score: float = Field(
        default=0.0,
        ge=0.0,
        le=100.0,
        description="% of context actually used in output"
    )

    # Cost Tracking
    cost_usd: float = Field(default=0.0, ge=0.0, description="Estimated cost in USD")

    # Metadata
    model_name: str = Field(default="unknown", description="LLM model used")
    operation_type: str = Field(default="unknown", description="Type of operation")
    language: Optional[str] = Field(default=None, description="Programming language if applicable")

    @computed_field
    @property
    def total_input_tokens(self) -> int:
        """Total tokens sent to the model."""
        return self.prompt_tokens + self.context_tokens + self.user_input_tokens - self.cached_tokens

    @computed_field
    @property
    def total_output_tokens(self) -> int:
        """Total tokens generated by the model."""
        return self.completion_tokens

    @computed_field
    @property
    def total_tokens(self) -> int:
        """Total tokens consumed (input + output)."""
        return self.total_input_tokens + self.total_output_tokens

    @computed_field
    @property
    def efficiency_score(self) -> float:
        """
        Quality per token consumed.

        Higher is better: indicates getting good results with fewer tokens.
        Returns 0 if no tokens consumed.
        """
        if self.total_tokens == 0:
            return 0.0
        return (self.output_quality_score / self.total_tokens) * 100

    @computed_field
    @property
    def cache_hit_rate(self) -> float:
        """
        Percentage of input tokens served from cache.

        Returns value between 0 and 100.
        """
        total_potential = self.prompt_tokens + self.context_tokens + self.user_input_tokens
        if total_potential == 0:
            return 0.0
        return (self.cached_tokens / total_potential) * 100

    @computed_field
    @property
    def context_efficiency(self) -> float:
        """
        How efficiently context tokens contributed to output quality.

        High score means context was valuable; low score suggests wasted context.
        """
        if self.context_tokens == 0:
            return 0.0
        return (self.context_utilization_score * self.output_quality_score) / 100


class SessionTokenSummary(BaseModel):
    """Aggregated token metrics for an entire training session."""

    session_id: str
    agent_id: str
    start_time: datetime
    end_time: Optional[datetime] = None

    # Aggregated Counts
    total_operations: int = 0
    total_prompt_tokens: int = 0
    total_context_tokens: int = 0
    total_user_input_tokens: int = 0
    total_completion_tokens: int = 0
    total_cached_tokens: int = 0

    # Aggregated Quality
    average_quality_score: float = 0.0
    average_context_utilization: float = 0.0
    average_efficiency_score: float = 0.0

    # Cost
    total_cost_usd: float = 0.0

    # Operations breakdown
    operations_by_type: Dict[str, int] = Field(default_factory=dict)

    @computed_field
    @property
    def total_input_tokens(self) -> int:
        """Total input tokens across all operations."""
        return (
            self.total_prompt_tokens
            + self.total_context_tokens
            + self.total_user_input_tokens
            - self.total_cached_tokens
        )

    @computed_field
    @property
    def total_tokens(self) -> int:
        """Total tokens consumed in session."""
        return self.total_input_tokens + self.total_completion_tokens

    @computed_field
    @property
    def average_cache_hit_rate(self) -> float:
        """Average cache hit rate across session."""
        total_potential = (
            self.total_prompt_tokens
            + self.total_context_tokens
            + self.total_user_input_tokens
        )
        if total_potential == 0:
            return 0.0
        return (self.total_cached_tokens / total_potential) * 100

    @computed_field
    @property
    def cost_per_operation(self) -> float:
        """Average cost per operation."""
        if self.total_operations == 0:
            return 0.0
        return self.total_cost_usd / self.total_operations


class AgentTokenStats(BaseModel):
    """Long-term token usage statistics for an agent."""

    agent_id: str
    total_sessions: int = 0
    total_operations: int = 0

    # Lifetime totals
    lifetime_tokens: int = 0
    lifetime_input_tokens: int = 0
    lifetime_output_tokens: int = 0
    lifetime_cost_usd: float = 0.0

    # Averages
    avg_tokens_per_session: float = 0.0
    avg_tokens_per_operation: float = 0.0
    avg_quality_score: float = 0.0
    avg_efficiency_score: float = 0.0
    avg_cache_hit_rate: float = 0.0

    # Trends (last 30 days)
    recent_sessions: List[str] = Field(default_factory=list)
    token_trend: List[int] = Field(default_factory=list, description="Daily token usage")
    quality_trend: List[float] = Field(default_factory=list, description="Daily quality scores")

    # Best/Worst
    best_efficiency_session: Optional[str] = None
    best_efficiency_score: float = 0.0
    worst_efficiency_session: Optional[str] = None
    worst_efficiency_score: float = 0.0

    @computed_field
    @property
    def cost_per_session(self) -> float:
        """Average cost per training session."""
        if self.total_sessions == 0:
            return 0.0
        return self.lifetime_cost_usd / self.total_sessions

    @computed_field
    @property
    def roi_score(self) -> float:
        """
        Return on investment score.

        Quality improvement per dollar spent.
        """
        if self.lifetime_cost_usd == 0:
            return 0.0
        return self.avg_quality_score / self.lifetime_cost_usd


class TokenBudget(BaseModel):
    """Token budget configuration for an operation or pattern."""

    operation_type: str
    max_tokens: int = Field(gt=0, description="Maximum total tokens allowed")
    max_context_tokens: int = Field(gt=0, description="Maximum context tokens")
    max_completion_tokens: int = Field(gt=0, description="Maximum output tokens")

    # Soft limits (warnings)
    warn_at_tokens: Optional[int] = Field(default=None, description="Warn when exceeding this")
    warn_at_cost_usd: Optional[float] = Field(default=None, description="Warn when exceeding this cost")

    # Optimization hints
    allow_caching: bool = Field(default=True, description="Enable prompt caching")
    allow_context_compression: bool = Field(default=True, description="Enable context compression")
    min_quality_score: float = Field(default=70.0, ge=0.0, le=100.0, description="Minimum acceptable quality")

    def is_within_budget(self, metrics: TokenMetrics) -> bool:
        """Check if token usage is within budget."""
        return (
            metrics.total_tokens <= self.max_tokens
            and metrics.context_tokens <= self.max_context_tokens
            and metrics.completion_tokens <= self.max_completion_tokens
        )

    def is_near_budget(self, metrics: TokenMetrics, threshold: float = 0.9) -> bool:
        """Check if token usage is approaching budget limits."""
        return (
            metrics.total_tokens >= self.max_tokens * threshold
            or metrics.context_tokens >= self.max_context_tokens * threshold
            or metrics.completion_tokens >= self.max_completion_tokens * threshold
        )

    def should_warn(self, metrics: TokenMetrics) -> bool:
        """Check if usage exceeds warning thresholds."""
        if self.warn_at_tokens and metrics.total_tokens > self.warn_at_tokens:
            return True
        if self.warn_at_cost_usd and metrics.cost_usd > self.warn_at_cost_usd:
            return True
        return False


class TokenOptimizationSuggestion(BaseModel):
    """Suggestion for optimizing token usage."""

    suggestion_id: str
    session_id: str
    operation_id: str

    # Issue
    issue_type: str = Field(description="Type of inefficiency detected")
    severity: str = Field(description="low, medium, high, critical")
    description: str = Field(description="Human-readable issue description")

    # Current state
    current_tokens: int
    current_cost_usd: float
    current_quality_score: float

    # Proposed improvement
    suggested_action: str
    estimated_tokens_saved: int
    estimated_cost_saved_usd: float
    estimated_quality_impact: float = Field(description="+/- change to quality score")

    # Implementation
    priority: int = Field(ge=1, le=5, description="1=highest, 5=lowest")
    automated: bool = Field(default=False, description="Can be applied automatically")
    code_example: Optional[str] = Field(default=None, description="Example implementation")


# Pricing constants (update as needed)
PRICING_PER_1K_TOKENS = {
    "gpt-4": {"input": 0.03, "output": 0.06},
    "gpt-4-turbo": {"input": 0.01, "output": 0.03},
    "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015},
    "claude-3-opus": {"input": 0.015, "output": 0.075},
    "claude-3-sonnet": {"input": 0.003, "output": 0.015},
    "claude-3-haiku": {"input": 0.00025, "output": 0.00125},
    "claude-sonnet-4-5": {"input": 0.003, "output": 0.015},
}


def calculate_cost(
    input_tokens: int,
    output_tokens: int,
    model_name: str = "claude-sonnet-4-5"
) -> float:
    """
    Calculate cost in USD for token usage.

    Args:
        input_tokens: Number of input tokens
        output_tokens: Number of output tokens
        model_name: Name of the model (default: claude-sonnet-4-5)

    Returns:
        Cost in USD
    """
    pricing = PRICING_PER_1K_TOKENS.get(model_name, {"input": 0.003, "output": 0.015})

    input_cost = (input_tokens / 1000) * pricing["input"]
    output_cost = (output_tokens / 1000) * pricing["output"]

    return input_cost + output_cost
